{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import euclidean\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizations Maximum\n",
    "\n",
    "### Maximal Population\n",
    "- vehicles with max population coverage  \n",
    "- we don’t look if pop is counted multiple times  \n",
    "- we could e.g. choose different routes + then sum the pop  \n",
    "- now we should only sum **unique** pop  \n",
    "\n",
    "### PROBLEM WITH `crs28922` | UNIQUE\n",
    "- we can go deeper and always take different routes → more unique people  \n",
    "- this is fine for now; discussion: quantity vs. quality  \n",
    "\n",
    "### Maximal Elderly Population\n",
    "population at risk (Age > 65)\n",
    "\n",
    "### Maximal Young Population\n",
    "population at risk (Age < 15)\n",
    "\n",
    "### Maximal Dutch Population\n",
    "population as control (Dutch)\n",
    "\n",
    "### Maximal Non-Western Migration Population\n",
    "population at risk (Non-Western Migration)\n",
    "\n",
    "### OLD Percentages\n",
    "…  \n",
    "\n",
    "### NON-WESTERN Percentages\n",
    "…  \n",
    "\n",
    "### Maximal Measurements\n",
    "Points Counts Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_top10(gdf: gpd.GeoDataFrame, n: int = 10) -> dict:\n",
    "    \"\"\"\n",
    "    Return a dict of the top-n rows for various metrics,\n",
    "    computing required percentages first.\n",
    "    Keys are 'max_<metric>'.\n",
    "    \"\"\"\n",
    "    df = gdf.copy()\n",
    "\n",
    "    # compute percentages\n",
    "    df['P_65+'] = df['A_65+'] / df['A_inhab'] * 100\n",
    "    df['P_n_west_m'] = df['A_n_west_m'] / df['A_inhab'] * 100\n",
    "\n",
    "    metrics = [\n",
    "        'A_inhab',\n",
    "        'A_65+',\n",
    "        'A_0_15',\n",
    "        'A_nederlan',\n",
    "        'A_n_west_m',\n",
    "        'P_65+',\n",
    "        'P_n_west_m',\n",
    "        'count'\n",
    "    ]\n",
    "\n",
    "    return {f'max_{m}': df.nlargest(n, m) for m in metrics}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sun Pop Nunique with CBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tops_with_cbs(\n",
    "    tops: Dict[str, gpd.GeoDataFrame],\n",
    "    cbs_gdf: gpd.GeoDataFrame,\n",
    "    list_col: str = 'crs28922_list',\n",
    "    code_col: str = 'crs28992'\n",
    ") -> Tuple[\n",
    "    Dict[str, List[str]],       # cbs_lists\n",
    "    Dict[str, int],             # max_number\n",
    "    Dict[str, gpd.GeoDataFrame] # gdf_filtered\n",
    "]:\n",
    "    \"\"\"\n",
    "    For each optimization in `tops`:\n",
    "      1) extract unique CBS codes from its top‐n GeoDataFrame\n",
    "      2) filter cbs_gdf by those codes (always returns CBS cells)\n",
    "      3) sum the appropriate column:\n",
    "         - for 'max_point_count' → sum the vehicle 'count'\n",
    "         - otherwise → sum the matching A_… column in CBS\n",
    "\n",
    "    Returns three dicts keyed by metric name:\n",
    "      - cbs_lists    : list of unique CRS codes\n",
    "      - max_number   : summed total (int)\n",
    "      - gdf_filtered : GeoDataFrame of matched CBS cells\n",
    "    \"\"\"\n",
    "    cbs_lists:    Dict[str, List[str]]           = {}\n",
    "    max_number:   Dict[str, int]                 = {}\n",
    "    gdf_filtered: Dict[str, gpd.GeoDataFrame]    = {}\n",
    "\n",
    "    for metric, df in tops.items():\n",
    "        raw_col = metric.replace('max_', '')              # e.g. 'A_inhab', 'P_65+', or 'count'\n",
    "        sum_col = raw_col if not raw_col.startswith('P_') else raw_col.replace('P_', 'A_')\n",
    "\n",
    "        # 1) collect all unique CBS codes\n",
    "        codes = set()\n",
    "        for val in df[list_col]:\n",
    "            if isinstance(val, list):\n",
    "                codes.update(val)\n",
    "            elif isinstance(val, str):\n",
    "                inner = val.strip(\"[]\")\n",
    "                parts = [p.strip(\" '\\\"\") for p in inner.replace(\"', '\", \",\").split(\",\") if p.strip()]\n",
    "                codes.update(parts)\n",
    "        cbs_lists[metric] = list(codes)\n",
    "\n",
    "        # 2) filter CBS cells (always)\n",
    "        cells = cbs_gdf[cbs_gdf[code_col].isin(codes)].copy()\n",
    "        gdf_filtered[metric] = cells\n",
    "\n",
    "        # 3) compute total\n",
    "        if raw_col == 'count':\n",
    "            total = int(df['count'].sum())\n",
    "        else:\n",
    "            # sum the A_… column in the filtered CBS cells\n",
    "            total = int(cells[sum_col].sum())\n",
    "\n",
    "        max_number[metric] = total\n",
    "\n",
    "    return cbs_lists, max_number, gdf_filtered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: *** PAPER if needed - additional frequencies calcualtes points per CRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_length_metrics(df):\n",
    "    \"\"\"\n",
    "    Compute per-cell metrics, print averages±std, and return key columns.\n",
    "    \"\"\"\n",
    "    d = df.copy()\n",
    "    d['len'] = d['crs28922_list'].apply(len)\n",
    "    d['A_inhab_per_len_crs28992']     = d['A_inhab'] / d['len']\n",
    "    d['point_count_per_len_crs28992'] = d['count']   / d['len']\n",
    "\n",
    "    ai, pc = d['A_inhab_per_len_crs28992'], d['point_count_per_len_crs28992']\n",
    "    print(f\"Avg A_inhab/cell: {ai.mean():.2f} (Std {ai.std():.2f})\")\n",
    "    print(f\"Avg points/cell:   {pc.mean():.2f} (Std {pc.std():.2f})\")\n",
    "\n",
    "    return d[['uni_id','A_inhab_per_len_crs28992','point_count_per_len_crs28992']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: Create a Summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_df_from_tops(tops: dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a summary table where each column is an optimization name\n",
    "    and each row is one of the selected uni_id values.\n",
    "\n",
    "    Parameters:\n",
    "        tops: dict mapping optimization names (e.g. 'max_A_inhab')\n",
    "              to DataFrames that contain a 'uni_id' column\n",
    "\n",
    "    Returns:\n",
    "        summary_df: DataFrame with one column per optimization,\n",
    "                    rows are the uni_id lists lined up by index.\n",
    "    \"\"\"\n",
    "    # build a dict of lists\n",
    "    data = {opt: df['uni_id'].tolist() for opt, df in tops.items()}\n",
    "    # create the DataFrame\n",
    "    summary_df = pd.DataFrame(data)\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTION OPTMIZATION FAIRNESS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INPUT DATA: GBS with stats\n",
    "### INPUT DATA:  Vehicles Stats \n",
    "### OUTPUT: Optimized Maximal Columns DF\n",
    "### OUTPUT: Optimized Maximal Vehicles Lists\n",
    "### OUTPUT: Vehicles Stats for each optimization\n",
    "### OUTPUT: Unique Number of relevant column (e.g. count, A_inhab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "\n",
    "gdf = gpd.read_file(\"data/vehicles_1503.gpkg\")\n",
    "\n",
    "cbs = gpd.read_file(\"data/cbs_full.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_max_coverage_pipeline(\n",
    "    gdf: gpd.GeoDataFrame,\n",
    "    cbs_gdf: gpd.GeoDataFrame,\n",
    "    n: int = 10\n",
    "):\n",
    "    \"\"\"\n",
    "    1) Compute top-n vehicles per max_* metric\n",
    "    2) Analyze CBS coverage for each top-n set\n",
    "    3) Build a summary table of uni_id selections\n",
    "\n",
    "    Parameters:\n",
    "      - gdf     : vehicles GeoDataFrame\n",
    "      - cbs_gdf : CBS cells GeoDataFrame with 'crs28992' and A_* columns\n",
    "      - n       : number of top vehicles to pick for each metric\n",
    "\n",
    "    Returns:\n",
    "      - tops           : dict of GeoDataFrames (one per 'max_*')\n",
    "      - cbs_lists      : dict of lists of crs28992 codes\n",
    "      - max_number     : dict of summed totals\n",
    "      - gdf_filtered   : dict of filtered CBS GeoDataFrames\n",
    "      - summary_df     : DataFrame, rows = rank, cols = 'max_*' with uni_id\n",
    "    \"\"\"\n",
    "    # 1) pick top-n vehicles per metric\n",
    "    tops = compute_top10(gdf, n=n)\n",
    "\n",
    "    # 2) get CBS coverage info\n",
    "    cbs_lists, max_number, gdf_filtered = analyze_tops_with_cbs(tops, cbs_gdf)\n",
    "\n",
    "    # 3) make a summary table of uni_id lists\n",
    "    summary_df = create_summary_df_from_tops(tops)\n",
    "\n",
    "    return tops, cbs_lists, max_number, gdf_filtered, summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tops, cbs_lists, max_number, gdf_filtered, summary_df = run_max_coverage_pipeline(\n",
    "    gdf, cbs, n=10\n",
    ")\n",
    "\n",
    "max_A_inhab       = tops['max_A_inhab']\n",
    "max_A_old         = tops['max_A_65+']\n",
    "max_A_young       = tops['max_A_0_15']\n",
    "max_A_dutch       = tops['max_A_nederlan']\n",
    "max_A_non_western = tops['max_A_n_west_m']\n",
    "max_P_old         = tops['max_P_65+']\n",
    "max_P_non_western = tops['max_P_n_west_m']\n",
    "max_point_count   = tops['max_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage 4 - NOT FOR NOW\n",
    "\n",
    "#report_length_metrics(max_point_count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export as csv\n",
    "# summary_df.to_csv('data/optimized_pop_count_1503.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sensing_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
