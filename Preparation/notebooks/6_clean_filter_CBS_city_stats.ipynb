{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e54cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a5f951",
   "metadata": {},
   "source": [
    "### Add, clean, filter, fill NaN CBS\n",
    "\n",
    "- Source: https://www.cbs.nl/nl-nl/dossier/nederland-regionaal/geografische-data/kaart-van-100-meter-bij-100-meter-met-statistieken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc66f4c3",
   "metadata": {},
   "source": [
    "### Fill random values *** PAPER\n",
    "\n",
    "- if sum of all age less than A_inhab, random allocation for NaN from 0 to 4 until the value is reached. \n",
    "- if sum is already reached NaN is 0! \n",
    "- for migration percentages were counted based on A_inhab\n",
    "- where the sum is lower everthing was raised to match! \n",
    "- If the sum of the variables A_nederlan, A_west_mig, and A_n_west_m is less than A_inhab in a given row, the missing (NaN) values in these three variables are imputed such that their total equals A_inhab. When multiple values are missing in a row, the difference is distributed equally among them.\n",
    "- in case still if there was more or less values in sums of each migration or age group, random allocation was made to each colum to hit the value, limitation but also not as important since it's not a focus. realtie valeus should be the same still.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60106c4a",
   "metadata": {},
   "source": [
    "#### Fill-out values so that the sums are equal to A_inhab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f293dc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cbs = gpd.read_file(\"data/cbs_amsterdam_2021_clean.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fef4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_adjust_cbs(cbs: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Clean and adjust CBS population data so that age and migration columns\n",
    "    sum exactly to A_inhab (total inhabitants).\n",
    "\n",
    "    Steps:\n",
    "    1. Drop irrelevant columns\n",
    "    2. Fill NaNs and replace inf values\n",
    "    3. Round to integers\n",
    "    4. Adjust group counts to match A_inhab exactly\n",
    "    5. Recalculate and verify sums\n",
    "\n",
    "    Parameters:\n",
    "    - cbs : GeoDataFrame with CBS columns\n",
    "\n",
    "    Returns:\n",
    "    - Cleaned and corrected GeoDataFrame\n",
    "    \"\"\"\n",
    "    # rename columns in cbs migration\n",
    "    cbs = cbs.rename(columns={\n",
    "        'G_woz_woning': 'G_woz_woni',\n",
    "        'A_nederlan': 'A_nederlan',\n",
    "        'A_west_mig': 'A_west_mig',\n",
    "        'A_n_west_mig': 'A_n_west_m',\n",
    "    }) \t\t\t\n",
    "\n",
    "    # drop housing column if present\n",
    "    if 'A_woning' in cbs.columns:\n",
    "        cbs = cbs.drop(columns=['A_woning'])\n",
    "\n",
    "    # define groups\n",
    "    age_cols = ['A_0_15', 'A_15_25', 'A_25_45', 'A_45_65', 'A_65+']\n",
    "    mig_cols = ['A_nederlan', 'A_west_mig', 'A_n_west_m']\n",
    "\n",
    "    # fill missing and inf values\n",
    "    cbs[age_cols + mig_cols + ['A_inhab']] = cbs[age_cols + mig_cols + ['A_inhab']].fillna(0)\n",
    "    cbs = cbs.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "    # round to whole persons\n",
    "    cbs[age_cols + mig_cols] = cbs[age_cols + mig_cols].round(0)\n",
    "\n",
    "    # adjust totals\n",
    "    def adjust_columns_to_target(row, target_col, columns):\n",
    "        target = int(row[target_col])\n",
    "        for col in columns:\n",
    "            row[col] = int(row[col])\n",
    "        current_sum = sum(row[col] for col in columns)\n",
    "        diff = target - current_sum\n",
    "        if diff > 0:\n",
    "            while current_sum < target:\n",
    "                col = random.choice(columns)\n",
    "                row[col] += 1\n",
    "                current_sum += 1\n",
    "        elif diff < 0:\n",
    "            total = -diff\n",
    "            base, rem = divmod(total, len(columns))\n",
    "            for i, col in enumerate(columns):\n",
    "                row[col] -= base + (1 if i < rem else 0)\n",
    "        return row\n",
    "\n",
    "    cbs = cbs.apply(lambda r: adjust_columns_to_target(r, 'A_inhab', age_cols), axis=1)\n",
    "    cbs = cbs.apply(lambda r: adjust_columns_to_target(r, 'A_inhab', mig_cols), axis=1)\n",
    "\n",
    "    # recompute and enforce integer type\n",
    "    cbs['age_sum'] = cbs[age_cols].sum(axis=1)\n",
    "    cbs['migration_sum'] = cbs[mig_cols].sum(axis=1)\n",
    "    for col in age_cols + mig_cols + ['age_sum', 'migration_sum']:\n",
    "        cbs[col] = cbs[col].astype(int)\n",
    "\n",
    "    # validation\n",
    "    assert (cbs['age_sum'] == cbs['A_inhab']).all()\n",
    "    assert (cbs['migration_sum'] == cbs['A_inhab']).all()\n",
    "\n",
    "    return cbs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c35ad8c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cbs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m cbs_full = clean_and_adjust_cbs(\u001b[43mcbs\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'cbs' is not defined"
     ]
    }
   ],
   "source": [
    "cbs_full = clean_and_adjust_cbs(cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6142e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate total sums for each column\n",
    "# total_inhab = cbs_full['A_inhab'].sum()\n",
    "# total_age = cbs_full['age_sum'].sum()\n",
    "# total_names = cbs_full['migration_sum'].sum()\n",
    "\n",
    "# # Print results\n",
    "# print(f\"Total A_inhab:  {total_inhab}\")\n",
    "# print(f\"Total age_sum:   {total_age}\")\n",
    "# print(f\"Total names_sum: {total_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f511141",
   "metadata": {},
   "source": [
    "### Linear Regression simple to predict G_woz_woni - PAPER *** \n",
    "\n",
    "We applied a linear regression model to estimate missing values in the variable G_woz_woni (average property value per residence). The model was trained using demographic features including age groups and migration background counts, leveraging only rows with complete data. Predicted values were then used to impute missing entries, ensuring consistency for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f26d3074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "\n",
    "def impute_woz_with_regression(cbs_full: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Use linear regression to impute missing or invalid (negative) G_woz_woni values in CBS data.\n",
    "\n",
    "    Parameters:\n",
    "    - cbs_full : DataFrame with population and housing attributes\n",
    "\n",
    "    Returns:\n",
    "    - Updated DataFrame with G_woz_woni imputed and cleaned\n",
    "    \"\"\"\n",
    "    features = ['A_inhab', 'A_0_15', 'A_15_25', 'A_25_45', 'A_45_65', 'A_65+',\n",
    "                'A_nederlan', 'A_west_mig', 'A_n_west_m']\n",
    "    target = 'G_woz_woni'\n",
    "\n",
    "    # Split known and missing target values\n",
    "    known = cbs_full[cbs_full[target].notna()]\n",
    "    missing = cbs_full[cbs_full[target].isna()]\n",
    "\n",
    "    # Train regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(known[features], known[target])\n",
    "\n",
    "    # Predict missing\n",
    "    if not missing.empty:\n",
    "        preds = model.predict(missing[features])\n",
    "        cbs_full.loc[cbs_full[target].isna(), target] = preds\n",
    "\n",
    "    # Replace negative values with mean of non-negative entries\n",
    "    mean_woz = cbs_full[cbs_full[target] >= 0][target].mean()\n",
    "    cbs_full.loc[cbs_full[target] < 0, target] = mean_woz\n",
    "\n",
    "    return cbs_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abf7bffc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cbs_full' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m cbs_full = impute_woz_with_regression(\u001b[43mcbs_full\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'cbs_full' is not defined"
     ]
    }
   ],
   "source": [
    "cbs_full = impute_woz_with_regression(cbs_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce056c9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cbs_full' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Iterate over each column and count negative values\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m columns_to_check:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     negative_counts[column] = (\u001b[43mcbs_full\u001b[49m[column] < \u001b[32m0\u001b[39m).sum()\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Display the counts\u001b[39;00m\n\u001b[32m     12\u001b[39m negative_counts\n",
      "\u001b[31mNameError\u001b[39m: name 'cbs_full' is not defined"
     ]
    }
   ],
   "source": [
    "# List of columns to check\n",
    "columns_to_check = ['A_0_15', 'A_15_25', 'A_25_45', 'A_45_65', 'A_65+', 'G_woz_woni', 'A_nederlan', 'A_west_mig', 'A_n_west_m']\n",
    "\n",
    "# Dictionary to store counts of negative values\n",
    "negative_counts = {}\n",
    "\n",
    "# Iterate over each column and count negative values\n",
    "for column in columns_to_check:\n",
    "    negative_counts[column] = (cbs_full[column] < 0).sum()\n",
    "\n",
    "# Display the counts\n",
    "negative_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933b6fde",
   "metadata": {},
   "source": [
    "### FUNCTION | Adjust NEgative Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c965edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_negative_values(cbs_full: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Detect and correct negative values in age and migration columns while preserving total group sums.\n",
    "    Negative values are set to 0 and their deficit is redistributed across the remaining positive columns.\n",
    "\n",
    "    Parameters:\n",
    "    - cbs_full : DataFrame with population columns\n",
    "\n",
    "    Returns:\n",
    "    - Cleaned DataFrame with non-negative age/migration columns and preserved group totals\n",
    "    \"\"\"\n",
    "\n",
    "    # Define column groups\n",
    "    age_cols = ['A_0_15', 'A_15_25', 'A_25_45', 'A_45_65', 'A_65+']\n",
    "    mig_cols = ['A_nederlan', 'A_west_mig', 'A_n_west_m']\n",
    "\n",
    "    def adjust_negatives(row, cols):\n",
    "        deficit = sum(-row[col] for col in cols if row[col] < 0)\n",
    "        original_sum = sum(row[col] for col in cols)\n",
    "        for col in cols:\n",
    "            if row[col] < 0:\n",
    "                row[col] = 0\n",
    "        while deficit > 0:\n",
    "            positive_cols = [col for col in cols if row[col] > 0]\n",
    "            if not positive_cols:\n",
    "                break\n",
    "            chosen_col = random.choice(positive_cols)\n",
    "            row[chosen_col] -= 1\n",
    "            deficit -= 1\n",
    "        return row\n",
    "\n",
    "    # Apply to migration and age columns\n",
    "    cbs_full = cbs_full.apply(lambda row: adjust_negatives(row, mig_cols), axis=1)\n",
    "    cbs_full = cbs_full.apply(lambda row: adjust_negatives(row, age_cols), axis=1)\n",
    "\n",
    "    # Recompute and verify\n",
    "    cbs_full['migration_sum'] = cbs_full[mig_cols].sum(axis=1)\n",
    "    cbs_full['age_sum'] = cbs_full[age_cols].sum(axis=1)\n",
    "    print(cbs_full[['A_inhab', 'migration_sum', 'age_sum']].head())\n",
    "\n",
    "    return cbs_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dac156ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cbs_full' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m cbs_full = adjust_negative_values(\u001b[43mcbs_full\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'cbs_full' is not defined"
     ]
    }
   ],
   "source": [
    "cbs_full = adjust_negative_values(cbs_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ea7d4c",
   "metadata": {},
   "source": [
    "## Final FUNCTION \n",
    "- For Amsterdam cleaned, filled, predicted, for space - \n",
    "- Ready for use\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685f4e12",
   "metadata": {},
   "source": [
    "### INPUT DATA: CBS SEMI CLEANED \n",
    "### OUTPUT DATA: CBS FULLY CLEANED "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1da6a8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_cbs_pipeline(cbs: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Pipeline to process CBS data:\n",
    "    1. Clean and adjust population groups\n",
    "    2. Impute missing G_woz_woni values\n",
    "    3. Adjust any remaining negative values\n",
    "    \"\"\"\n",
    "    cbs_clean = clean_and_adjust_cbs(cbs)\n",
    "    cbs_imputed = impute_woz_with_regression(cbs_clean)\n",
    "    cbs_full = adjust_negative_values(cbs_imputed)\n",
    "    return cbs_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "343799a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "DataSourceError",
     "evalue": "data/cbs_amsterdam_2021_clean.shp: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDataSourceError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m cbs = \u001b[43mgpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/cbs_amsterdam_2021_clean.shp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m full_cbs = process_cbs_pipeline(cbs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pkoljensic\\OneDrive - Delft University of Technology\\Desktop\\PYTHON\\projects\\sensing\\sensing_env\\Lib\\site-packages\\geopandas\\io\\file.py:294\u001b[39m, in \u001b[36m_read_file\u001b[39m\u001b[34m(filename, bbox, mask, columns, rows, engine, **kwargs)\u001b[39m\n\u001b[32m    291\u001b[39m             from_bytes = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mpyogrio\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_file_pyogrio\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mfiona\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    299\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pd.api.types.is_file_like(filename):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pkoljensic\\OneDrive - Delft University of Technology\\Desktop\\PYTHON\\projects\\sensing\\sensing_env\\Lib\\site-packages\\geopandas\\io\\file.py:547\u001b[39m, in \u001b[36m_read_file_pyogrio\u001b[39m\u001b[34m(path_or_bytes, bbox, mask, rows, **kwargs)\u001b[39m\n\u001b[32m    538\u001b[39m     warnings.warn(\n\u001b[32m    539\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[33m'\u001b[39m\u001b[33minclude_fields\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mignore_fields\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keywords are deprecated, and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    540\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwill be removed in a future release. You can use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keyword \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    543\u001b[39m         stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    544\u001b[39m     )\n\u001b[32m    545\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m] = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33minclude_fields\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpyogrio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pkoljensic\\OneDrive - Delft University of Technology\\Desktop\\PYTHON\\projects\\sensing\\sensing_env\\Lib\\site-packages\\pyogrio\\geopandas.py:265\u001b[39m, in \u001b[36mread_dataframe\u001b[39m\u001b[34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, fid_as_index, use_arrow, on_invalid, arrow_to_pandas_kwargs, **kwargs)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_arrow:\n\u001b[32m    261\u001b[39m     \u001b[38;5;66;03m# For arrow, datetimes are read as is.\u001b[39;00m\n\u001b[32m    262\u001b[39m     \u001b[38;5;66;03m# For numpy IO, datetimes are read as string values to preserve timezone info\u001b[39;00m\n\u001b[32m    263\u001b[39m     \u001b[38;5;66;03m# as numpy does not directly support timezones.\u001b[39;00m\n\u001b[32m    264\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mdatetime_as_string\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m result = \u001b[43mread_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgdal_force_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfid_as_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_arrow:\n\u001b[32m    285\u001b[39m     meta, table = result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pkoljensic\\OneDrive - Delft University of Technology\\Desktop\\PYTHON\\projects\\sensing\\sensing_env\\Lib\\site-packages\\pyogrio\\raw.py:198\u001b[39m, in \u001b[36mread\u001b[39m\u001b[34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, return_fids, datetime_as_string, **kwargs)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Read OGR data source into numpy arrays.\u001b[39;00m\n\u001b[32m     60\u001b[39m \n\u001b[32m     61\u001b[39m \u001b[33;03mIMPORTANT: non-linear geometry types (e.g., MultiSurface) are converted\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    194\u001b[39m \n\u001b[32m    195\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    196\u001b[39m dataset_kwargs = _preprocess_options_key_value(kwargs) \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mogr_read\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_vsi_path_or_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_mask_to_wkb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatetime_as_string\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatetime_as_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpyogrio\\\\_io.pyx:1240\u001b[39m, in \u001b[36mpyogrio._io.ogr_read\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpyogrio\\\\_io.pyx:220\u001b[39m, in \u001b[36mpyogrio._io.ogr_open\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mDataSourceError\u001b[39m: data/cbs_amsterdam_2021_clean.shp: No such file or directory"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "cbs = gpd.read_file(\"data/cbs_amsterdam_2021_clean.shp\")\n",
    "\n",
    "full_cbs = process_cbs_pipeline(cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbf2dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cbs_full.to_csv('cbs_full.csv', index=False)\n",
    "# cbs_full.to_file('cbs_full.shp', driver='ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3a3e33",
   "metadata": {},
   "source": [
    "### FINAL Function 2 \n",
    "\n",
    "- Create city stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6e675e",
   "metadata": {},
   "source": [
    "### INPUT DATA: CBS CITY CLEANED FILTERED FULL \n",
    "### OUTPUT DATA: CITY STATS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf4a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_city_stats(cbs_city):\n",
    "    \"\"\"\n",
    "    Compute city-level demographic and housing statistics from CBS data.\n",
    "\n",
    "    Outputs:\n",
    "    - Total inhabitants\n",
    "    - Mean G_woz_woni (property value)\n",
    "    - Absolute sums for age and migration groups\n",
    "    - Relative percentages for age and migration groups\n",
    "    - Returns result as a one-row DataFrame\n",
    "    \"\"\"\n",
    "    total_inhab = cbs_city['A_inhab'].sum()\n",
    "    mean_woz = round(cbs_city['G_woz_woni'].mean(), 2)\n",
    "\n",
    "    # absolute sums\n",
    "    age_cols = ['A_0_15', 'A_15_25', 'A_25_45', 'A_45_65', 'A_65+']\n",
    "    mig_cols = ['A_nederlan', 'A_west_mig', 'A_n_west_m']\n",
    "    age_sums = cbs_city[age_cols].sum()\n",
    "    mig_sums = cbs_city[mig_cols].sum()\n",
    "\n",
    "    # calculate and round percentages\n",
    "    pct_age = (age_sums / total_inhab * 100).round(2)\n",
    "    pct_mig = (mig_sums / total_inhab * 100).round(2)\n",
    "\n",
    "    stats = pd.DataFrame([{\n",
    "        'Area': 'Amsterdam',\n",
    "        'A_inhab': total_inhab,\n",
    "        'G_woz_woni': mean_woz,\n",
    "        **age_sums.to_dict(),\n",
    "        **mig_sums.to_dict(),\n",
    "        'P_0_15': pct_age['A_0_15'],\n",
    "        'P_15_25': pct_age['A_15_25'],\n",
    "        'P_25_45': pct_age['A_25_45'],\n",
    "        'P_45_65': pct_age['A_45_65'],\n",
    "        'P_65+': pct_age['A_65+'],\n",
    "        'P_nederlan': pct_mig['A_nederlan'],\n",
    "        'P_west_mig': pct_mig['A_west_mig'],\n",
    "        'P_n_west_m': pct_mig['A_n_west_m'],\n",
    "    }])\n",
    "\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a558d60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_city_stats(cbs_city):\n",
    "\n",
    "#      \"\"\"\n",
    "#     Compute city-level demographic and housing statistics from CBS data.\n",
    "\n",
    "#     Outputs:\n",
    "#     - Total inhabitants\n",
    "#     - Mean G_woz_woni (property value)\n",
    "#     - Absolute sums for age and migration groups\n",
    "#     - Relative percentages for age and migration groups\n",
    "#     - Returns result as a one-row DataFrame\n",
    "#     \"\"\"\n",
    "#     total_inhab  = cbs_city['A_inhab'].sum()\n",
    "#     mean_woz     = round(cbs_city['G_woz_woni'].mean(), 2)\n",
    "\n",
    "#     # absolute sums\n",
    "#     age_cols = ['A_0_15','A_15_25','A_25_45','A_45_65','A_65+']\n",
    "#     mig_cols = ['A_nederlan','A_west_mig','A_n_west_m']\n",
    "#     age_sums = cbs_city[age_cols].sum()\n",
    "#     mig_sums = cbs_city[mig_cols].sum()\n",
    "\n",
    "#     # calculate and round percentages\n",
    "#     pct_age = (age_sums / total_inhab * 100).round(2)\n",
    "#     pct_mig = (mig_sums / total_inhab * 100).round(2)\n",
    "\n",
    "#     stats = pd.DataFrame([{\n",
    "#         'Area':        'Amsterdam',\n",
    "#         'A_inhab':     total_inhab,\n",
    "#         'G_woz_woni':  mean_woz,\n",
    "#         **age_sums.to_dict(),\n",
    "#         **mig_sums.to_dict(),\n",
    "#         'P_0_15':      pct_age['A_0_15'],\n",
    "#         'P_15_25':     pct_age['A_15_25'],\n",
    "#         'P_25_45':     pct_age['A_25_45'],\n",
    "#         'P_45_65':     pct_age['A_45_65'],\n",
    "#         'P_65+':       pct_age['A_65+'],\n",
    "#         'P_nederlan':  pct_mig['A_nederlan'],\n",
    "#         'P_west_mig':  pct_mig['A_west_mig'],\n",
    "#         'P_n_west_m':  pct_mig['A_n_west_m'],\n",
    "#     }])\n",
    "\n",
    "#     return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "921b60cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Usage\n",
    "cbs_city = compute_city_stats(full_cbs) # export from before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "da552b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>A_inhab</th>\n",
       "      <th>G_woz_woni</th>\n",
       "      <th>A_0_15</th>\n",
       "      <th>A_15_25</th>\n",
       "      <th>A_25_45</th>\n",
       "      <th>A_45_65</th>\n",
       "      <th>A_65+</th>\n",
       "      <th>A_nederlan</th>\n",
       "      <th>A_west_mig</th>\n",
       "      <th>A_n_west_m</th>\n",
       "      <th>P_0_15</th>\n",
       "      <th>P_15_25</th>\n",
       "      <th>P_25_45</th>\n",
       "      <th>P_45_65</th>\n",
       "      <th>P_65+</th>\n",
       "      <th>P_nederlan</th>\n",
       "      <th>P_west_mig</th>\n",
       "      <th>P_n_west_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>870375.0</td>\n",
       "      <td>469.16</td>\n",
       "      <td>122560</td>\n",
       "      <td>108267</td>\n",
       "      <td>317106</td>\n",
       "      <td>210257</td>\n",
       "      <td>112185</td>\n",
       "      <td>382143</td>\n",
       "      <td>172607</td>\n",
       "      <td>315625</td>\n",
       "      <td>14.08</td>\n",
       "      <td>12.44</td>\n",
       "      <td>36.43</td>\n",
       "      <td>24.16</td>\n",
       "      <td>12.89</td>\n",
       "      <td>43.91</td>\n",
       "      <td>19.83</td>\n",
       "      <td>36.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Area   A_inhab  G_woz_woni  A_0_15  A_15_25  A_25_45  A_45_65   A_65+  \\\n",
       "0  Amsterdam  870375.0      469.16  122560   108267   317106   210257  112185   \n",
       "\n",
       "   A_nederlan  A_west_mig  A_n_west_m  P_0_15  P_15_25  P_25_45  P_45_65  \\\n",
       "0      382143      172607      315625   14.08    12.44    36.43    24.16   \n",
       "\n",
       "   P_65+  P_nederlan  P_west_mig  P_n_west_m  \n",
       "0  12.89       43.91       19.83       36.26  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbs_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "03fb995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cbs_city.to_csv(\"cbs_stats_amsterdam.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sensing_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
